{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c80123e",
   "metadata": {},
   "outputs": [],
   "source": [
    "% pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b50d46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from xgboost import XGBRegressor\n",
    "import optuna\n",
    "import pandas as pd\n",
    "\n",
    "# Importing excel sheets as dfs, should both have samples in the same order\n",
    "predictors_df = pd.read_excel('Predictors_Cleaned.xlsx', 0)\n",
    "    # Rows are samples, columns are predictors\n",
    "outcomes_df = pd.read_excel('Outcomes_Cleaned.xlsx', 0)\n",
    "    # Rows are samples, columns are mechanisms\n",
    "\n",
    "# Dropping unnecessary metadata\n",
    "X = predictors_df.drop('SAMPLE NAME', axis=1)\n",
    "Y = outcomes_df.drop('MECHANISM', axis=1) # Full DF must iterate through\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, random_state=42, shuffle=True, test_size=0.3)\n",
    "\n",
    "x_trainval, x_valid, y_trainval, y_valid = train_test_split(x_train, y_train, shuffle=True,  random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ae9f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "\n",
    "def xgb_objective(trial):\n",
    "    param = {\n",
    "        'tree_method':'gpu_hist',  # uses GPU for faster training\n",
    "        'sampling_method': 'gradient_based',\n",
    "        'lambda': trial.suggest_loguniform('lambda', 7.0, 17.0),\n",
    "        'alpha': trial.suggest_loguniform('alpha', 7.0, 17.0),\n",
    "        'eta': trial.suggest_categorical('eta', [0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]),\n",
    "        'gamma': trial.suggest_categorical('gamma', [18, 19, 20, 21, 22, 23, 24, 25]),\n",
    "        'learning_rate': trial.suggest_categorical('learning_rate', [0.008,0.01,0.012,0.014,0.016,0.018, 0.02]),\n",
    "        'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.3,0.4,0.5,0.6,0.7,0.8,0.9, 1.0]),\n",
    "        'colsample_bynode': trial.suggest_categorical('colsample_bynode', [0.3,0.4,0.5,0.6,0.7,0.8,0.9, 1.0]),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 400, 1000),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 8, 600),  \n",
    "        'max_depth': trial.suggest_categorical('max_depth', [3, 4, 5, 6, 7]),  \n",
    "        'subsample': trial.suggest_categorical('subsample', [0.5,0.6,0.7,0.8,1.0]),\n",
    "        'random_state': 42\n",
    "    }\n",
    "\n",
    "    model = XGBRegressor(**param)  \n",
    "    \n",
    "    model.fit(x_trainval, y_trainval, eval_set=[(x_valid, y_valid)], early_stopping_rounds=10, verbose=False)\n",
    "    \n",
    "    predict = model.predict(x_valid)\n",
    "    \n",
    "    r_2 = abs(r2_score(predict, y_valid))\n",
    "    \n",
    "    return r_2\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(xgb_objective, n_trials=50, show_progress_bar=True)\n",
    "    \n",
    "print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))\n",
    "\n",
    "    # https://www.kaggle.com/code/alisultanov/regression-xgboost-optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68f5e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import numpy as np\n",
    "\n",
    "def rf_objective(trial):\n",
    "    # Hyperparameters\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 10, 200, log=True)\n",
    "    max_depth = trial.suggest_int(\"max_depth\", 2, 32)\n",
    "    min_samples_split = trial.suggest_int(\"min_samples_split\", 2, 10)\n",
    "    min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 1, 10)\n",
    "\n",
    "    # Model random forest\n",
    "    model = RandomForestRegressor(\n",
    "    n_estimators=n_estimators,\n",
    "    max_depth=max_depth,\n",
    "    min_samples_split=min_samples_split,\n",
    "    min_samples_leaf=min_samples_leaf,\n",
    "    random_state=42,\n",
    "    )\n",
    "    model.fit(x_train, y_train)\n",
    "\n",
    "    # Calculate r2\n",
    "    y_pred = model.predict(x_test)\n",
    "    r2 = abs(r2_score(y_test, y_pred))\n",
    "\n",
    "    return r2\n",
    "\n",
    "# Create study object\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(rf_objective, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "print(\"Best trial:\", study.best_trial)\n",
    "print(\"Best hyperparameters:\", study.best_params)\n",
    "\n",
    "# https://www.kaggle.com/code/mustafagerme/optimization-of-random-forest-model-using-optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e6fdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "def lasso_objective(trial):\n",
    "\n",
    "    # Hyperparams\n",
    "    _alpha = trial.suggest_float(\"alpha\", 0.0001, 0.01)\n",
    "\n",
    "    # Model Lasso\n",
    "    lasso = linear_model.Lasso(alpha=_alpha, random_state=0)\n",
    "    lasso = lasso.fit(x_train, y_train)\n",
    "\n",
    "    # Calculating r2\n",
    "    pred = lasso.predict(x_test)\n",
    "    r2 = abs(r2_score(y_test, pred))\n",
    "\n",
    "    return r2\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(lasso_objective, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "print(\"Best trial:\", study.best_trial)\n",
    "print(\"Best hyperparameters:\", study.best_params)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
